{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exam 1 - Luis Alberto Robles Hernandez.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPQ6ZLNZ+9Q92eF5IqsKeg6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisRobles18/NLP/blob/main/Exam_1_Luis_Alberto_Robles_Hernandez.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDSYI3fJwfT_"
      },
      "source": [
        "# **Exam 1**\r\n",
        "**Student ID:** 002581393 **Name:** Luis Alberto Robles Hernandez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gscM716xJjI"
      },
      "source": [
        "##**INSTRUCTIONS:** \r\n",
        "Using the provided movie reviews dataset, please write code to answer the following questions.\r\n",
        "The dataset has two classes: positive and negative (TRAINING folder), and the files for each\r\n",
        "are properly labeled in the respective folders (positive, negative). Each txt document includes\r\n",
        "one single review. The UNLABELED folder is for evaluation and should only be used then, not\r\n",
        "for training. I will rerun all your notebook code to make sure you did not use it for training.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCRvBRgCxxOM"
      },
      "source": [
        "**Styling notes:**\r\n",
        "\r\n",
        "1) Do not load any python packages/libraries inside any of the functions. You can have these in separate code cells, but not inside any given function.\r\n",
        "\r\n",
        "2) Do not print anything from within your functions, this is bad form.\r\n",
        "\r\n",
        "3) Make sure you pay attention to what the function is required to return.\r\n",
        "\r\n",
        "4) If not explicitly requested, please print out whatever the function is supposed to return.\r\n",
        "\r\n",
        "5) When printing to screen to show your results, make sure you format your output nicely and make sure it makes sense.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCAv7FnJwW-N"
      },
      "source": [
        "from IPython.display import clear_output\r\n",
        "#Downloading the Shakespeare's work to the root folder\r\n",
        "!wget https://github.com/LuisRobles18/NLP/blob/main/exam1_dataset.zip?raw=true -O exam1_dataset.zip\r\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS3jZwBeyIp-"
      },
      "source": [
        "##**EXERCISE 1**\r\n",
        "**1. (20 points)** Write a generic function that takes: Classification algorithm name, vectorization method name, training set with labels as parameters (total of 3 parameters should be passed). The function should take the classification algorithm name, the vectorization methodâ€™s name, and the training set and train the desired model. Use the default training parameters for the models we have seen in class. This function should return the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPGWQBCxzllJ"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "\r\n",
        "def train_model(alg_name, vec_name, training_set):\r\n",
        "\r\n",
        "    if alg_name.lower() == \"multinomial nb\":\r\n",
        "    \r\n",
        "        if vec_name.lower() == \"tfidf\":\r\n",
        "        \r\n",
        "            model = make_pipeline(TfidfVectorizer(), MultinomialNB())\r\n",
        "            model.fit(training_set['data'], training_set['target'])\r\n",
        "            return model\r\n",
        "        \r\n",
        "        if vec_name.lower() == \"count vectorizer\":\r\n",
        "        \r\n",
        "            model = make_pipeline(CountVectorizer(), MultinomialNB())\r\n",
        "            model.fit(training_set['data'], training_set['target'])\r\n",
        "            return model\r\n",
        "        \r\n",
        "        return -1\r\n",
        "\r\n",
        "    elif alg_name.lower() == \"random forest\":\r\n",
        "    \r\n",
        "        if vec_name.lower() == \"tfidf\":\r\n",
        "        \r\n",
        "            model = make_pipeline(TfidfVectorizer(), RandomForestClassifier())\r\n",
        "            model.fit(training_set['data'], training_set['target'])\r\n",
        "            return model\r\n",
        "        \r\n",
        "        if vec_name.lower() == \"count vectorizer\":\r\n",
        "        \r\n",
        "            model = make_pipeline(CountVectorizer(), RandomForestClassifier())\r\n",
        "            model.fit(training_set['data'], training_set['target'])\r\n",
        "            return model\r\n",
        "        \r\n",
        "        return -1\r\n",
        "      \r\n",
        "    elif alg_name.lower() == \"support vector machines\":\r\n",
        "    \r\n",
        "        if vec_name.lower() == \"tfidf\":\r\n",
        "        \r\n",
        "            model = make_pipeline(TfidfVectorizer(), SVC())\r\n",
        "            model.fit(training_set['data'], training_set['target'])\r\n",
        "            return model\r\n",
        "        \r\n",
        "        if vec_name.lower() == \"count vectorizer\":\r\n",
        "        \r\n",
        "            model = make_pipeline(CountVectorizer(), SVC())\r\n",
        "            model.fit(training_set['data'], training_set['target'])\r\n",
        "            return model\r\n",
        "        \r\n",
        "        return -1\r\n",
        "        \r\n",
        "    \r\n",
        "    else:\r\n",
        "\r\n",
        "        return -1\r\n",
        "        \r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FjYZTQY1eUU"
      },
      "source": [
        "##**EXERCISE 2**\r\n",
        "\r\n",
        "**2. (30 points)** Using the function from question 1 to build the following models:\r\n",
        "\r\n",
        "a) Model a: Naive Bayes, Vectorizer: TFIDF and Bag of Words, Training set should be 75% of the provided dataset. Leaving the remaining 25% for testing.\r\n",
        "\r\n",
        "b) Model b: RandomForest, Vectorizer: TFIDF and Bag of Words, Training set should be 70% of the provided dataset. Leaving the remaining 30% for testing.\r\n",
        "\r\n",
        "c) Model c: Support Vector Machines (SVC in sklearn), Vectorizer: TFIDF and Bag of Words, Training set should be 60% of the provided dataset. Leaving the remaining 40% for testing.\r\n",
        "\r\n",
        "**NOTE: Set the random seed to: 12345. This needs to be consistently set to train the model AND split the data in test and train. If this is not done correctly, you will lose points as your answers will not be comparable with the grading key.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI5ks2iz4O_P",
        "outputId": "9abc71bd-7269-41e1-9ecd-e6ab80b8727a"
      },
      "source": [
        "import zipfile\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "#This random seed is to have the same results with the grading key\r\n",
        "random.seed(12345)\r\n",
        "np.random.seed(12345)\r\n",
        "\r\n",
        "#Openning the dataset ZIP file\r\n",
        "my_zip = zipfile.ZipFile('exam1_dataset.zip')\r\n",
        "storage_path = '.'\r\n",
        "\r\n",
        "#------------------OBTAINING THE DATASETS--------------------------\r\n",
        "data = []\r\n",
        "target = []\r\n",
        "target_names = [\"negative_reviews\",\"positive_reviews\"]\r\n",
        "documents_filename = []\r\n",
        "\r\n",
        "for file in my_zip.namelist():\r\n",
        "    #We will extract the negative reviews first to the data\r\n",
        "    if file.startswith('exam1_dataset/TRAINING/negative/') and file.endswith('.txt'):\r\n",
        "        documents_filename.append(os.path.basename(my_zip.getinfo(file).filename))\r\n",
        "        with my_zip.open(file,\"r\") as doc:\r\n",
        "            #Since the files read are from a zip file, we must convert them from binary to string using the decode function\r\n",
        "            data.append(doc.read().decode())\r\n",
        "            #For negative reviews, the value for the target will be 0\r\n",
        "            target.append(0)\r\n",
        "\r\n",
        "for file in my_zip.namelist():\r\n",
        "    #Then we will extract the positive reviews to the data\r\n",
        "    if file.startswith('exam1_dataset/TRAINING/positive/') and file.endswith('.txt'):\r\n",
        "        documents_filename.append(os.path.basename(my_zip.getinfo(file).filename))\r\n",
        "        with my_zip.open(file,\"r\") as doc:\r\n",
        "            #Since the files read are from a zip file, we must convert them from binary to string using the decode function\r\n",
        "            data.append(doc.read().decode())\r\n",
        "            #For positive reviews, the value for the target will be 1\r\n",
        "            target.append(1)\r\n",
        "\r\n",
        "#================ MODEL A ===================\r\n",
        "print(\"Building Model A...\")\r\n",
        "#Model = Multinomial NB\r\n",
        "#% of Training = 75%\r\n",
        "#% of Testing = 25%\r\n",
        "\r\n",
        "model_a_training_data, model_a_test_data, model_a_training_target, model_a_test_target = train_test_split(data, target, test_size=0.25, random_state = 12345)\r\n",
        "model_a_training_set = {}\r\n",
        "model_a_training_set['data'] =  model_a_training_data\r\n",
        "model_a_training_set['target'] = model_a_training_target\r\n",
        "\r\n",
        "print(\"Building Model A using TF-IDF...\")\r\n",
        "#Vectorizer = TF-IDF\r\n",
        "model_a_nb_tfidf = train_model(\"Multinomial NB\", \"TFIDF\", model_a_training_set)\r\n",
        "\r\n",
        "print(\"Building Model A using CountVectorizer...\")\r\n",
        "#Vectorizer = CountVectorizer\r\n",
        "model_a_nb_cv = train_model(\"Multinomial NB\", \"Count Vectorizer\", model_a_training_set)\r\n",
        "\r\n",
        "#================ MODEL B ===================\r\n",
        "print(\"Building Model B...\")\r\n",
        "#Model = Random Forest\r\n",
        "#% of Training = 70%\r\n",
        "#% of Testing = 30%\r\n",
        "\r\n",
        "model_b_training_data, model_b_test_data, model_b_training_target, model_b_test_target = train_test_split(data, target, test_size=0.30, random_state = 12345)\r\n",
        "model_b_training_set = {}\r\n",
        "model_b_training_set['data'] =  model_b_training_data\r\n",
        "model_b_training_set['target'] = model_b_training_target\r\n",
        "\r\n",
        "print(\"Building Model B using TF-IDF...\")\r\n",
        "#Vectorizer = TF-IDF\r\n",
        "model_b_rf_tfidf = train_model(\"Random Forest\", \"TFIDF\", model_b_training_set)\r\n",
        "\r\n",
        "print(\"Building Model B using CountVectorizer...\")\r\n",
        "#Vectorizer = CountVectorizer\r\n",
        "model_b_rf_cv = train_model(\"Random Forest\", \"Count Vectorizer\", model_b_training_set)\r\n",
        "\r\n",
        "#================ MODEL C ===================\r\n",
        "print(\"Building Model C...\")\r\n",
        "#Model = Support Vector Machines\r\n",
        "#% of Training = 60%\r\n",
        "#% of Testing = 40%\r\n",
        "\r\n",
        "model_c_training_data, model_c_test_data, model_c_training_target, model_c_test_target = train_test_split(data, target, test_size=0.40, random_state = 12345)\r\n",
        "model_c_training_set = {}\r\n",
        "model_c_training_set['data'] =  model_c_training_data\r\n",
        "model_c_training_set['target'] = model_c_training_target\r\n",
        "\r\n",
        "print(\"Building Model C using TF-IDF...\")\r\n",
        "#Vectorizer = TF-IDF\r\n",
        "model_c_svm_tfidf = train_model(\"Support Vector Machines\", \"TFIDF\", model_c_training_set)\r\n",
        "\r\n",
        "print(\"Building Model C using CountVectorizer...\")\r\n",
        "#Vectorizer = CountVectorizer\r\n",
        "model_c_svm_cv = train_model(\"Support Vector Machines\", \"Count Vectorizer\", model_c_training_set)\r\n",
        "\r\n",
        "print(\"Success\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Model A...\n",
            "Building Model A using TF-IDF...\n",
            "Building Model A using CountVectorizer...\n",
            "Building Model B...\n",
            "Building Model B using TF-IDF...\n",
            "Building Model B using CountVectorizer...\n",
            "Building Model C...\n",
            "Building Model C using TF-IDF...\n",
            "Building Model C using CountVectorizer...\n",
            "Success\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaCJ82UwbPVF"
      },
      "source": [
        "##**EXERCISE 3**\r\n",
        "\r\n",
        "**3. (30 points)** Using the models from Question 2, evaluate each model with its respective testing set (for model a, that set is 25% of the data, for model b, that set is 30% of the data, and for model c that set is 40% of the data. Be careful to not mix up the evaluation sets. With the predictions on the test set and show the following metrics: Accuracy, Precision, Recall, and Macro F1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "YDSCGCCMbsnD",
        "outputId": "213b7e1d-c2a5-48ee-d488-b90c742e2e32"
      },
      "source": [
        "import sklearn\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.DataFrame(columns=['Model Name','Training %','Test %','Accuracy','Precision','Recall','Macro F1-Score'])\r\n",
        "\r\n",
        "#With these two variables we will determine which of the following models performs the best\r\n",
        "best_accuracy = -1.0\r\n",
        "best_model = None\r\n",
        "\r\n",
        "print(\"Evaluating and getting metrics from Multinomial NB TF-IDF...\")\r\n",
        "#====== Multinomial NB TF-IDF (METRICS) ============\r\n",
        "model_a_nb_tfidf_labels = model_a_nb_tfidf.predict(model_a_test_data)\r\n",
        "model_a_nb_tfidf_accuracy = sklearn.metrics.accuracy_score(model_a_test_target,model_a_nb_tfidf_labels)\r\n",
        "model_a_nb_tfidf_precision = sklearn.metrics.precision_score(model_a_test_target,model_a_nb_tfidf_labels)\r\n",
        "model_a_nb_tfidf_recall = sklearn.metrics.recall_score(model_a_test_target,model_a_nb_tfidf_labels)\r\n",
        "model_a_nb_tfidf_f1 = sklearn.metrics.f1_score(model_a_nb_tfidf_labels, model_a_test_target, average='macro')\r\n",
        "\r\n",
        "model_a_nb_tfidf_metrics = ['Multinomial NB (TF-IDF)','75%','25%',model_a_nb_tfidf_accuracy,model_a_nb_tfidf_precision,model_a_nb_tfidf_recall,model_a_nb_tfidf_f1]\r\n",
        "\r\n",
        "if model_a_nb_tfidf_accuracy > best_accuracy:\r\n",
        "    best_model = model_a_nb_tfidf\r\n",
        "    best_accuracy = model_a_nb_tfidf_accuracy\r\n",
        "\r\n",
        "print(\"Evaluating and getting metrics from Multinomial Count Vectorizer...\")\r\n",
        "#====== Multinomial NB Count Vectorizer (METRICS) ============\r\n",
        "model_a_nb_cv_labels = model_a_nb_cv.predict(model_a_test_data)\r\n",
        "model_a_nb_cv_accuracy = sklearn.metrics.accuracy_score(model_a_test_target,model_a_nb_cv_labels)\r\n",
        "model_a_nb_cv_precision = sklearn.metrics.precision_score(model_a_test_target,model_a_nb_cv_labels)\r\n",
        "model_a_nb_cv_recall = sklearn.metrics.recall_score(model_a_test_target,model_a_nb_cv_labels)\r\n",
        "model_a_nb_cv_f1 = sklearn.metrics.f1_score(model_a_nb_cv_labels, model_a_test_target, average='macro')\r\n",
        "\r\n",
        "model_a_nb_cv_metrics = ['Multinomial NB (Count Vectorizer)','75%','25%',model_a_nb_cv_accuracy,model_a_nb_cv_precision,model_a_nb_cv_recall,model_a_nb_cv_f1]\r\n",
        "\r\n",
        "if model_a_nb_cv_accuracy > best_accuracy:\r\n",
        "    best_model = model_a_nb_cv\r\n",
        "    best_accuracy = model_a_nb_cv_accuracy\r\n",
        "\r\n",
        "print(\"Evaluating and getting metrics from Random Forest TF-IDF...\")\r\n",
        "#====== Random Forest TF-IDF (METRICS) ============\r\n",
        "model_b_rf_tfidf_labels = model_b_rf_tfidf.predict(model_b_test_data)\r\n",
        "model_b_rf_tfidf_accuracy = sklearn.metrics.accuracy_score(model_b_test_target,model_b_rf_tfidf_labels)\r\n",
        "model_b_rf_tfidf_precision = sklearn.metrics.precision_score(model_b_test_target,model_b_rf_tfidf_labels)\r\n",
        "model_b_rf_tfidf_recall = sklearn.metrics.recall_score(model_b_test_target,model_b_rf_tfidf_labels)\r\n",
        "model_b_rf_tfidf_f1 = sklearn.metrics.f1_score(model_b_rf_tfidf_labels, model_b_test_target, average='macro')\r\n",
        "\r\n",
        "model_b_rf_tfidf_metrics = ['Random Forest (TF-IDF)','70%','30%',model_b_rf_tfidf_accuracy,model_b_rf_tfidf_precision,model_b_rf_tfidf_recall,model_b_rf_tfidf_f1]\r\n",
        "\r\n",
        "if model_b_rf_tfidf_accuracy > best_accuracy:\r\n",
        "    best_model = model_b_rf_tfidf\r\n",
        "    best_accuracy = model_b_rf_tfidf_accuracy\r\n",
        "\r\n",
        "print(\"Evaluating and getting metrics from Random Forest Count Vectorizer...\")\r\n",
        "#====== Random Forest Count Vectorizer (METRICS) ============\r\n",
        "model_b_rf_cv_labels = model_b_rf_cv.predict(model_b_test_data)\r\n",
        "model_b_rf_cv_accuracy = sklearn.metrics.accuracy_score(model_b_test_target,model_b_rf_cv_labels)\r\n",
        "model_b_rf_cv_precision = sklearn.metrics.precision_score(model_b_test_target,model_b_rf_cv_labels)\r\n",
        "model_b_rf_cv_recall = sklearn.metrics.recall_score(model_b_test_target,model_b_rf_cv_labels)\r\n",
        "model_b_rf_cv_f1 = sklearn.metrics.f1_score(model_b_rf_cv_labels, model_b_test_target, average='macro')\r\n",
        "\r\n",
        "model_b_rf_cv_metrics = ['Random Forest (Count Vectorizer)','70%','30%',model_b_rf_cv_accuracy,model_b_rf_cv_precision,model_b_rf_cv_recall,model_b_rf_cv_f1]\r\n",
        "\r\n",
        "if model_b_rf_cv_accuracy > best_accuracy:\r\n",
        "    best_model = model_b_rf_cv\r\n",
        "    best_accuracy = model_b_rf_cv_accuracy\r\n",
        "\r\n",
        "print(\"Evaluating and getting metrics from Support Vector Machines TF-IDF...\")\r\n",
        "#====== Support Vector Machines TF-IDF (METRICS) ============\r\n",
        "model_c_svm_tfidf_labels = model_c_svm_tfidf.predict(model_c_test_data)\r\n",
        "model_c_svm_tfidf_accuracy = sklearn.metrics.accuracy_score(model_c_test_target,model_c_svm_tfidf_labels)\r\n",
        "model_c_svm_tfidf_precision = sklearn.metrics.precision_score(model_c_test_target,model_c_svm_tfidf_labels)\r\n",
        "model_c_svm_tfidf_recall = sklearn.metrics.recall_score(model_c_test_target,model_c_svm_tfidf_labels)\r\n",
        "model_c_svm_tfidf_f1 = sklearn.metrics.f1_score(model_c_svm_tfidf_labels, model_c_test_target, average='macro')\r\n",
        "\r\n",
        "model_c_svm_tfidf_metrics = ['Support Vector Machines (TF-IDF)','60%','40%',model_c_svm_tfidf_accuracy,model_c_svm_tfidf_precision,model_c_svm_tfidf_recall,model_c_svm_tfidf_f1]\r\n",
        "\r\n",
        "if model_c_svm_tfidf_accuracy > best_accuracy:\r\n",
        "    best_model = model_c_svm_tfidf\r\n",
        "    best_accuracy = model_c_svm_tfidf_accuracy\r\n",
        "\r\n",
        "print(\"Evaluating and getting metrics from Support Vector Machines Count Vectorizer...\")\r\n",
        "#====== Support Vector Machines Count Vectorizer (METRICS) ============\r\n",
        "model_c_svm_cv_labels = model_c_svm_cv.predict(model_c_test_data)\r\n",
        "model_c_svm_cv_accuracy = sklearn.metrics.accuracy_score(model_c_test_target,model_c_svm_cv_labels)\r\n",
        "model_c_svm_cv_precision = sklearn.metrics.precision_score(model_c_test_target,model_c_svm_cv_labels)\r\n",
        "model_c_svm_cv_recall = sklearn.metrics.recall_score(model_c_test_target,model_c_svm_cv_labels)\r\n",
        "model_c_svm_cv_f1 = sklearn.metrics.f1_score(model_c_svm_cv_labels, model_c_test_target, average='macro')\r\n",
        "\r\n",
        "model_c_svm_cv_metrics = ['Support Vector Machines (Count Vectorizer)','60%','40%',model_c_svm_cv_accuracy,model_c_svm_cv_precision,model_c_svm_cv_recall,model_c_svm_cv_f1]\r\n",
        "\r\n",
        "if model_c_svm_cv_accuracy > best_accuracy:\r\n",
        "    best_model = model_c_svm_cv\r\n",
        "    best_accuracy = model_c_svm_cv_accuracy\r\n",
        "\r\n",
        "print(\"Success!\")\r\n",
        "\r\n",
        "#Adding the previous metrics to a dataframe\r\n",
        "\r\n",
        "df.loc[len(df), :] = model_a_nb_tfidf_metrics\r\n",
        "df.loc[len(df), :] = model_a_nb_cv_metrics\r\n",
        "df.loc[len(df), :] = model_b_rf_tfidf_metrics\r\n",
        "df.loc[len(df), :] = model_b_rf_cv_metrics\r\n",
        "df.loc[len(df), :] = model_c_svm_tfidf_metrics\r\n",
        "df.loc[len(df), :] = model_c_svm_cv_metrics\r\n",
        "\r\n",
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating and getting metrics from Multinomial NB TF-IDF...\n",
            "Evaluating and getting metrics from Multinomial Count Vectorizer...\n",
            "Evaluating and getting metrics from Random Forest TF-IDF...\n",
            "Evaluating and getting metrics from Random Forest Count Vectorizer...\n",
            "Evaluating and getting metrics from Support Vector Machines TF-IDF...\n",
            "Evaluating and getting metrics from Support Vector Machines Count Vectorizer...\n",
            "Success!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Name</th>\n",
              "      <th>Training %</th>\n",
              "      <th>Test %</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Macro F1-Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Multinomial NB (TF-IDF)</td>\n",
              "      <td>75%</td>\n",
              "      <td>25%</td>\n",
              "      <td>0.86896</td>\n",
              "      <td>0.87711</td>\n",
              "      <td>0.850966</td>\n",
              "      <td>0.868774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Multinomial NB (Count Vectorizer)</td>\n",
              "      <td>75%</td>\n",
              "      <td>25%</td>\n",
              "      <td>0.84992</td>\n",
              "      <td>0.871444</td>\n",
              "      <td>0.812643</td>\n",
              "      <td>0.849448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Random Forest (TF-IDF)</td>\n",
              "      <td>70%</td>\n",
              "      <td>30%</td>\n",
              "      <td>0.839467</td>\n",
              "      <td>0.841349</td>\n",
              "      <td>0.828525</td>\n",
              "      <td>0.839343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Random Forest (Count Vectorizer)</td>\n",
              "      <td>70%</td>\n",
              "      <td>30%</td>\n",
              "      <td>0.849867</td>\n",
              "      <td>0.839552</td>\n",
              "      <td>0.857376</td>\n",
              "      <td>0.849852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Support Vector Machines (TF-IDF)</td>\n",
              "      <td>60%</td>\n",
              "      <td>40%</td>\n",
              "      <td>0.8959</td>\n",
              "      <td>0.888162</td>\n",
              "      <td>0.903753</td>\n",
              "      <td>0.8959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Support Vector Machines (Count Vectorizer)</td>\n",
              "      <td>60%</td>\n",
              "      <td>40%</td>\n",
              "      <td>0.8541</td>\n",
              "      <td>0.839581</td>\n",
              "      <td>0.872276</td>\n",
              "      <td>0.854084</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Model Name  ... Macro F1-Score\n",
              "0                     Multinomial NB (TF-IDF)  ...       0.868774\n",
              "1           Multinomial NB (Count Vectorizer)  ...       0.849448\n",
              "2                      Random Forest (TF-IDF)  ...       0.839343\n",
              "3            Random Forest (Count Vectorizer)  ...       0.849852\n",
              "4            Support Vector Machines (TF-IDF)  ...         0.8959\n",
              "5  Support Vector Machines (Count Vectorizer)  ...       0.854084\n",
              "\n",
              "[6 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5fl4DGObhlT"
      },
      "source": [
        "With this in mind, please write and answer these questions in your\r\n",
        "notebook:\r\n",
        "\r\n",
        "**a) What model performs the best and why? (which metrics do you base this on, and why do you think it performs better than others).**\r\n",
        "\r\n",
        "Support Vector Machines, using the TF-IDF vectorizer, because it has the highest accuracy comparing to the other models (according to the previous results from above). I think I would perform better than others, because, once predicting labels using the testing set, the margin of error would be smaller compared to the other models.\r\n",
        "\r\n",
        "**b) Why is it important not to mix up the testing sets between different models? Think about this one.**\r\n",
        "\r\n",
        "Because there could be some cases, that some of the elements from the testing set, could insertect with the elements trained with a specific model, so the results wouldn't be reliable, that is, the metrics obtained would not represent the correct accuracy.\r\n",
        "\r\n",
        "**c) Display in a single sorted dataframe (model name, training %, test %, accuracy, precision, recall, F1-score) all performance metrics, sorted by accuracy in descending manner.**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "lAIe54anvOW-",
        "outputId": "6a20726a-d6fe-4e0d-bbb0-04c8352c395f"
      },
      "source": [
        "df.sort_values(by='Accuracy', ascending=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Name</th>\n",
              "      <th>Training %</th>\n",
              "      <th>Test %</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Macro F1-Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Support Vector Machines (TF-IDF)</td>\n",
              "      <td>60%</td>\n",
              "      <td>40%</td>\n",
              "      <td>0.8959</td>\n",
              "      <td>0.888162</td>\n",
              "      <td>0.903753</td>\n",
              "      <td>0.8959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Multinomial NB (TF-IDF)</td>\n",
              "      <td>75%</td>\n",
              "      <td>25%</td>\n",
              "      <td>0.86896</td>\n",
              "      <td>0.87711</td>\n",
              "      <td>0.850966</td>\n",
              "      <td>0.868774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Support Vector Machines (Count Vectorizer)</td>\n",
              "      <td>60%</td>\n",
              "      <td>40%</td>\n",
              "      <td>0.8541</td>\n",
              "      <td>0.839581</td>\n",
              "      <td>0.872276</td>\n",
              "      <td>0.854084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Multinomial NB (Count Vectorizer)</td>\n",
              "      <td>75%</td>\n",
              "      <td>25%</td>\n",
              "      <td>0.84992</td>\n",
              "      <td>0.871444</td>\n",
              "      <td>0.812643</td>\n",
              "      <td>0.849448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Random Forest (Count Vectorizer)</td>\n",
              "      <td>70%</td>\n",
              "      <td>30%</td>\n",
              "      <td>0.849867</td>\n",
              "      <td>0.839552</td>\n",
              "      <td>0.857376</td>\n",
              "      <td>0.849852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Random Forest (TF-IDF)</td>\n",
              "      <td>70%</td>\n",
              "      <td>30%</td>\n",
              "      <td>0.839467</td>\n",
              "      <td>0.841349</td>\n",
              "      <td>0.828525</td>\n",
              "      <td>0.839343</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Model Name  ... Macro F1-Score\n",
              "4            Support Vector Machines (TF-IDF)  ...         0.8959\n",
              "0                     Multinomial NB (TF-IDF)  ...       0.868774\n",
              "5  Support Vector Machines (Count Vectorizer)  ...       0.854084\n",
              "1           Multinomial NB (Count Vectorizer)  ...       0.849448\n",
              "3            Random Forest (Count Vectorizer)  ...       0.849852\n",
              "2                      Random Forest (TF-IDF)  ...       0.839343\n",
              "\n",
              "[6 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrt27QAkxSJK"
      },
      "source": [
        "##**EXERCISE 4**\r\n",
        "\r\n",
        "**4. (15 points)** Using the documents in the folder named UNLABELED, please use\r\n",
        "your best performing trained model from question 3 to predict their labels. Please do this individually for each document. Print to the screen the following items: Document Name, Predicted Label.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "r7bQmKdkxgJa",
        "outputId": "bcb534de-7515-4b08-d4b4-4712bb922a9c"
      },
      "source": [
        "#Openning the dataset ZIP file\r\n",
        "my_zip = zipfile.ZipFile('exam1_dataset.zip')\r\n",
        "storage_path = '.'\r\n",
        "\r\n",
        "#------------------OBTAINING THE UNLABELED DATASET--------------------------\r\n",
        "data_unlabeled = []\r\n",
        "documents_name = []\r\n",
        "\r\n",
        "for file in my_zip.namelist():\r\n",
        "    #We will extract the unlabeled txt files from the ZIP file\r\n",
        "    if file.startswith('exam1_dataset/UNLABELED/') and file.endswith('.txt'):\r\n",
        "        documents_name.append(my_zip.getinfo(file).filename)\r\n",
        "        with my_zip.open(file,\"r\") as doc:\r\n",
        "            #Since the files read are from a zip file, we must convert them from binary to string using the decode function\r\n",
        "            data_unlabeled.append(doc.read().decode())\r\n",
        "\r\n",
        "#We define a method in which returns the predicted label\r\n",
        "def predict_category(s, target_n, model_temp):\r\n",
        "    pred = model_temp.predict([s])\r\n",
        "    return target_n[pred[0]]\r\n",
        "\r\n",
        "#We create a Pandas Dataframe\r\n",
        "df_prediction = pd.DataFrame(columns=['Document_Name','Predicted_Label'])\r\n",
        "\r\n",
        "#We will iterate over each document from the UNLABELED folder, with their respective filename and predicted label\r\n",
        "#The best performing model is located in the best_model variable obtained from the previous exercise\r\n",
        "for i in range(len(data_unlabeled)):\r\n",
        "\r\n",
        "    temp_list = [os.path.basename(documents_name[i]), predict_category(data_unlabeled[i],target_names,best_model)]\r\n",
        "    df_prediction.loc[len(df_prediction), :] = temp_list\r\n",
        "\r\n",
        "df_prediction\r\n",
        "\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document_Name</th>\n",
              "      <th>Predicted_Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0_0.txt</td>\n",
              "      <td>positive_reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24221_0.txt</td>\n",
              "      <td>negative_reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35968_0.txt</td>\n",
              "      <td>negative_reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>35991_0.txt</td>\n",
              "      <td>negative_reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>36022_0.txt</td>\n",
              "      <td>negative_reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>36149_0.txt</td>\n",
              "      <td>negative_reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>36517_0.txt</td>\n",
              "      <td>negative_reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>37154_0.txt</td>\n",
              "      <td>negative_reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>46278_0.txt</td>\n",
              "      <td>negative_reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>46705_0.txt</td>\n",
              "      <td>negative_reviews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>49990_0.txt</td>\n",
              "      <td>negative_reviews</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Document_Name   Predicted_Label\n",
              "0        0_0.txt  positive_reviews\n",
              "1    24221_0.txt  negative_reviews\n",
              "2    35968_0.txt  negative_reviews\n",
              "3    35991_0.txt  negative_reviews\n",
              "4    36022_0.txt  negative_reviews\n",
              "5    36149_0.txt  negative_reviews\n",
              "6    36517_0.txt  negative_reviews\n",
              "7    37154_0.txt  negative_reviews\n",
              "8    46278_0.txt  negative_reviews\n",
              "9    46705_0.txt  negative_reviews\n",
              "10   49990_0.txt  negative_reviews"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgSMoRRHxgTn"
      },
      "source": [
        "**Using a text cell, write your own opinion if the label is correct and why. You have to read the document to make your own opinion.**\r\n",
        "After reading all the unlabeled reviews, it seems every single file is a negative review. As we can see, there is one prediction with a wrong label. The reason is because, if we remember from the metrics from the best model, its accuracy is almost 90%. So, that means that on average, out of 10 predictions, the model may be wrong in one of them, as seen in the previous results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrl9uFaq1I7i"
      },
      "source": [
        "##**EXERCISE 5**\r\n",
        "\r\n",
        "**5. (20 points)** Build a function that takes the set of documents as input and returns a cosine similarity matrix for those documents. Feed all documents in the TRAINING folder to this matrix. Instead of printing the returned cosine similarity matrix, create a heatmap plot from the returned matrix. Make sure your plot is nicely scaled, properly labeled, and uses a nice color range to show the similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRcpVthG1awy"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import numpy as np;\r\n",
        "\r\n",
        "np.random.seed(12345)\r\n",
        "\r\n",
        "def generate_cos_sim_matrix(documents):\r\n",
        "\r\n",
        "    #We call the TF-IDF from the Sklearn module\r\n",
        "    tfidf_vectorizer = TfidfVectorizer()\r\n",
        "\r\n",
        "    #We generate the tf-idf vectors for the text files\r\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\r\n",
        "\r\n",
        "    total = len(documents)\r\n",
        "    final_cosine_sim_matrix = [None] * total\r\n",
        "\r\n",
        "    #In order to avoid memory getting full quickly, we will do the analysis per row\r\n",
        "    for i in range(total):\r\n",
        "        final_cosine_sim_matrix[i] = cosine_similarity(tfidf_matrix[i], tfidf_matrix).tolist()\r\n",
        "    return final_cosine_sim_matrix\r\n",
        "\r\n",
        "\r\n",
        "cos_similarity_matrix = generate_cos_sim_matrix(data)\r\n",
        "\r\n",
        "#We reshape to a 25000 by 25000 matrix in a numpy array\r\n",
        "a = np.array(cos_similarity_matrix)\r\n",
        "new_cos_similarity_matrix = a.reshape(25000,25000)\r\n",
        "del cos_similarity_matrix #Delete from memory old list"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCB06FmpGCV9"
      },
      "source": [
        "**Showing the heat map from the previous matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk6-hD9KFUGM"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "plt.imshow(new_cos_similarity_matrix, cmap='hot', interpolation='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B33RbUeAaFIY"
      },
      "source": [
        "##**EXERCISE 6**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu54_B5CaJNa"
      },
      "source": [
        "**6. (15 points)** Write a function that takes a cosine similarity matrix as input and returns a list with the top n document paris and their similarity. Note that you should only keep the document pairs that are unique and remove the comparisons of the document to itself. Print the top 50 similar document pairs.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq2SsDknaPTb"
      },
      "source": [
        "def top_n_similar_works(sim_matrix,top_n):\r\n",
        "    top_n_works = {}\r\n",
        "\r\n",
        "    #We iterate over each cell in the matrix\r\n",
        "    total = len(sim_matrix)\r\n",
        "    for i in range(len(sim_matrix)):\r\n",
        "        for j in range(len(sim_matrix[i])):\r\n",
        "\r\n",
        "            #Excludes cosine similarity with itself\r\n",
        "            if i != j:\r\n",
        "\r\n",
        "                if len(top_n_works) < top_n:\r\n",
        "                    top_n_works[str(i)+\",\"+str(j)] = sim_matrix[i][j]\r\n",
        "                else:\r\n",
        "                    is_mirror_sim = False\r\n",
        "                    for key in top_n_works:\r\n",
        "                        coordinates = key.split(\",\")\r\n",
        "                        #This condition is to exclude mirror similarities if found\r\n",
        "                        if(int(coordinates[1]) == i and int(coordinates[0]) == j):\r\n",
        "                            is_mirror_sim = True\r\n",
        "                    \r\n",
        "                    if is_mirror_sim == False:\r\n",
        "                        #If the current cell is greater than the lowest value of the top N list\r\n",
        "                        #It will be replaced with the current cell\r\n",
        "                        if top_n_works[min(top_n_works, key=top_n_works.get)] < sim_matrix[i][j]:\r\n",
        "                            del top_n_works[min(top_n_works, key=top_n_works.get)]\r\n",
        "                            top_n_works[str(i)+\",\"+str(j)] = sim_matrix[i][j]\r\n",
        "\r\n",
        "    #The function will return a dataframe with the top N similar works, including their similarity\r\n",
        "    df_top_similar = pd.DataFrame(columns=[\"[Position] Document\", \"[Position] Most similar document\", \"Similarity\"])\r\n",
        "    top_n_works = sorted(top_n_works.items(), key=lambda kv: kv[1], reverse=True)\r\n",
        "\r\n",
        "\r\n",
        "    for i in range(len(top_n_works)):\r\n",
        "        coordinates = top_n_works[i][0].split(\",\")\r\n",
        "        df_top_similar.loc[len(df_top_similar)] = [\"[\"+coordinates[0]+\"]\"+documents_filename[int(coordinates[0])],\r\n",
        "                                                   \"[\"+coordinates[1]+\"]\"+documents_filename[int(coordinates[1])],top_n_works[i][1]]\r\n",
        "\r\n",
        "    return df_top_similar.style.set_properties(**{'text-align': 'left'})"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUc77QApij7s"
      },
      "source": [
        "**Getting the Top 50 similar document pairs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPib59tXiqdG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5b8f166-8492-4de9-df86-e8b99fa511e7"
      },
      "source": [
        "#Get top 50 similar document pairs\r\n",
        "top_n_similar_works(new_cos_similarity_matrix,50)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_12278c00_7771_11eb_ad72_0242ac1c0002row0_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row0_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row0_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row1_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row1_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row1_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row2_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row2_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row2_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row3_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row3_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row3_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row4_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row4_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row4_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row5_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row5_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row5_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row6_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row6_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row6_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row7_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row7_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row7_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row8_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row8_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row8_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row9_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row9_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row9_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row10_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row10_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row10_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row11_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row11_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row11_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row12_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row12_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row12_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row13_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row13_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row13_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row14_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row14_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row14_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row15_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row15_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row15_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row16_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row16_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row16_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row17_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row17_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row17_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row18_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row18_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row18_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row19_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row19_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row19_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row20_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row20_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row20_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row21_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row21_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row21_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row22_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row22_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row22_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row23_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row23_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row23_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row24_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row24_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row24_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row25_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row25_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row25_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row26_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row26_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row26_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row27_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row27_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row27_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row28_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row28_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row28_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row29_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row29_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row29_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row30_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row30_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row30_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row31_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row31_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row31_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row32_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row32_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row32_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row33_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row33_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row33_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row34_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row34_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row34_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row35_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row35_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row35_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row36_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row36_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row36_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row37_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row37_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row37_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row38_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row38_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row38_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row39_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row39_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row39_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row40_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row40_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row40_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row41_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row41_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row41_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row42_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row42_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row42_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row43_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row43_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row43_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row44_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row44_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row44_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row45_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row45_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row45_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row46_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row46_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row46_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row47_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row47_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row47_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row48_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row48_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row48_col2,#T_12278c00_7771_11eb_ad72_0242ac1c0002row49_col0,#T_12278c00_7771_11eb_ad72_0242ac1c0002row49_col1,#T_12278c00_7771_11eb_ad72_0242ac1c0002row49_col2{\n",
              "            text-align:  left;\n",
              "        }</style><table id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >[Position] Document</th>        <th class=\"col_heading level0 col1\" >[Position] Most similar document</th>        <th class=\"col_heading level0 col2\" >Similarity</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row0_col0\" class=\"data row0 col0\" >[367]10330_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row0_col1\" class=\"data row0 col1\" >[4498]279_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row0_col2\" class=\"data row0 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row1_col0\" class=\"data row1 col0\" >[369]10332_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row1_col1\" class=\"data row1 col1\" >[4521]281_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row1_col2\" class=\"data row1 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row2_col0\" class=\"data row2 col0\" >[2148]11934_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row2_col1\" class=\"data row2 col1\" >[7532]552_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row3_col0\" class=\"data row3 col0\" >[6927]4986_2.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row3_col1\" class=\"data row3 col1\" >[10134]7872_2.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row3_col2\" class=\"data row3 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row4_col0\" class=\"data row4 col0\" >[6930]4989_3.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row4_col1\" class=\"data row4 col1\" >[10137]7875_3.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row4_col2\" class=\"data row4 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row5_col0\" class=\"data row5 col0\" >[8973]6827_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row5_col1\" class=\"data row5 col1\" >[9882]7645_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row5_col2\" class=\"data row5 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row6_col0\" class=\"data row6 col0\" >[15341]1307_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row6_col1\" class=\"data row6 col1\" >[15343]1309_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row6_col2\" class=\"data row6 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row7_col0\" class=\"data row7 col0\" >[23080]8273_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row7_col1\" class=\"data row7 col1\" >[23085]8278_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row7_col2\" class=\"data row7 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row8_col0\" class=\"data row8 col0\" >[370]10333_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row8_col1\" class=\"data row8 col1\" >[4532]282_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row8_col2\" class=\"data row8 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row9_col0\" class=\"data row9 col0\" >[1343]11209_3.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row9_col1\" class=\"data row9 col1\" >[2016]11815_3.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row9_col2\" class=\"data row9 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row10_col0\" class=\"data row10 col0\" >[1660]11495_2.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row10_col1\" class=\"data row10 col1\" >[7195]5226_2.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row10_col2\" class=\"data row10 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row11_col0\" class=\"data row11 col0\" >[1749]11575_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row11_col1\" class=\"data row11 col1\" >[3784]2156_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row11_col2\" class=\"data row11 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row12_col0\" class=\"data row12 col0\" >[2192]11974_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row12_col1\" class=\"data row12 col1\" >[7555]5550_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row12_col2\" class=\"data row12 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row13_col0\" class=\"data row13 col0\" >[2192]11974_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row13_col1\" class=\"data row13 col1\" >[8001]5952_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row13_col2\" class=\"data row13 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row14_col0\" class=\"data row14 col0\" >[2194]11976_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row14_col1\" class=\"data row14 col1\" >[7557]5552_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row14_col2\" class=\"data row14 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row15_col0\" class=\"data row15 col0\" >[2194]11976_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row15_col1\" class=\"data row15 col1\" >[8003]5954_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row15_col2\" class=\"data row15 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row16_col0\" class=\"data row16 col0\" >[2576]12319_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row16_col1\" class=\"data row16 col1\" >[9550]7346_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row16_col2\" class=\"data row16 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row17_col0\" class=\"data row17 col0\" >[4402]2712_2.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row17_col1\" class=\"data row17 col1\" >[7586]5579_2.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row17_col2\" class=\"data row17 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row18_col0\" class=\"data row18 col0\" >[4403]2713_2.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row18_col1\" class=\"data row18 col1\" >[7588]5580_2.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row18_col2\" class=\"data row18 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row19_col0\" class=\"data row19 col0\" >[4406]2716_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row19_col1\" class=\"data row19 col1\" >[7591]5583_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row19_col2\" class=\"data row19 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row20_col0\" class=\"data row20 col0\" >[4660]2945_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row20_col1\" class=\"data row20 col1\" >[4666]2950_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row20_col2\" class=\"data row20 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row21_col0\" class=\"data row21 col0\" >[5949]4104_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row21_col1\" class=\"data row21 col1\" >[12364]987_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row21_col2\" class=\"data row21 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row22_col0\" class=\"data row22 col0\" >[6925]4984_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row22_col1\" class=\"data row22 col1\" >[10132]7870_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row22_col2\" class=\"data row22 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row23_col0\" class=\"data row23 col0\" >[6952]5007_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row23_col1\" class=\"data row23 col1\" >[6953]5008_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row23_col2\" class=\"data row23 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row24_col0\" class=\"data row24 col0\" >[7036]5083_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row24_col1\" class=\"data row24 col1\" >[8766]6640_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row24_col2\" class=\"data row24 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row25_col0\" class=\"data row25 col0\" >[7041]5088_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row25_col1\" class=\"data row25 col1\" >[8771]6645_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row25_col2\" class=\"data row25 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row26_col0\" class=\"data row26 col0\" >[7555]5550_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row26_col1\" class=\"data row26 col1\" >[8001]5952_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row26_col2\" class=\"data row26 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row27_col0\" class=\"data row27 col0\" >[7557]5552_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row27_col1\" class=\"data row27 col1\" >[8003]5954_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row27_col2\" class=\"data row27 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row28_col0\" class=\"data row28 col0\" >[8971]6825_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row28_col1\" class=\"data row28 col1\" >[9880]7643_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row28_col2\" class=\"data row28 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row29_col0\" class=\"data row29 col0\" >[8974]6828_2.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row29_col1\" class=\"data row29 col1\" >[9883]7646_2.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row29_col2\" class=\"data row29 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row30_col0\" class=\"data row30 col0\" >[9243]706_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row30_col1\" class=\"data row30 col1\" >[9265]708_4.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row30_col2\" class=\"data row30 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row31_col0\" class=\"data row31 col0\" >[9261]7086_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row31_col1\" class=\"data row31 col1\" >[9267]7091_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row31_col2\" class=\"data row31 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row32_col0\" class=\"data row32 col0\" >[12694]10175_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row32_col1\" class=\"data row32 col1\" >[12697]10178_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row32_col2\" class=\"data row32 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row33_col0\" class=\"data row33 col0\" >[15618]1557_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row33_col1\" class=\"data row33 col1\" >[15619]1558_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row33_col2\" class=\"data row33 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row34_col0\" class=\"data row34 col0\" >[17363]3127_9.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row34_col1\" class=\"data row34 col1\" >[17372]3135_9.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row34_col2\" class=\"data row34 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row35_col0\" class=\"data row35 col0\" >[17569]3312_8.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row35_col1\" class=\"data row35 col1\" >[24622]9661_8.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row35_col2\" class=\"data row35 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row36_col0\" class=\"data row36 col0\" >[17570]3313_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row36_col1\" class=\"data row36 col1\" >[24623]9662_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row36_col2\" class=\"data row36 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row37_col0\" class=\"data row37 col0\" >[17571]3314_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row37_col1\" class=\"data row37 col1\" >[24624]9663_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row37_col2\" class=\"data row37 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row38_col0\" class=\"data row38 col0\" >[17573]3316_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row38_col1\" class=\"data row38 col1\" >[24626]9665_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row38_col2\" class=\"data row38 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row39_col0\" class=\"data row39 col0\" >[18625]4263_8.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row39_col1\" class=\"data row39 col1\" >[18629]4267_8.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row39_col2\" class=\"data row39 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row40_col0\" class=\"data row40 col0\" >[20258]5733_7.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row40_col1\" class=\"data row40 col1\" >[20262]5737_7.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row40_col2\" class=\"data row40 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row41_col0\" class=\"data row41 col0\" >[20631]6069_8.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row41_col1\" class=\"data row41 col1\" >[24242]9319_8.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row41_col2\" class=\"data row41 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row42_col0\" class=\"data row42 col0\" >[20900]6310_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row42_col1\" class=\"data row42 col1\" >[20909]6319_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row42_col2\" class=\"data row42 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row43_col0\" class=\"data row43 col0\" >[20962]6367_7.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row43_col1\" class=\"data row43 col1\" >[20963]6368_7.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row43_col2\" class=\"data row43 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row44_col0\" class=\"data row44 col0\" >[21054]644_9.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row44_col1\" class=\"data row44 col1\" >[21076]646_9.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row44_col2\" class=\"data row44 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row45_col0\" class=\"data row45 col0\" >[21479]6832_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row45_col1\" class=\"data row45 col1\" >[21480]6833_10.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row45_col2\" class=\"data row45 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row46_col0\" class=\"data row46 col0\" >[23503]8654_9.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row46_col1\" class=\"data row46 col1\" >[23506]8657_9.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row46_col2\" class=\"data row46 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row47_col0\" class=\"data row47 col0\" >[7040]5087_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row47_col1\" class=\"data row47 col1\" >[8770]6644_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row47_col2\" class=\"data row47 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row48_col0\" class=\"data row48 col0\" >[7042]5089_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row48_col1\" class=\"data row48 col1\" >[8772]6646_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row48_col2\" class=\"data row48 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row49_col0\" class=\"data row49 col0\" >[7556]5551_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row49_col1\" class=\"data row49 col1\" >[8002]5953_1.txt</td>\n",
              "                        <td id=\"T_12278c00_7771_11eb_ad72_0242ac1c0002row49_col2\" class=\"data row49 col2\" >1.000000</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f61ac9eb7d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo_5F7c-aPbE"
      },
      "source": [
        "Compare the assigned class for each document and answer:\r\n",
        "\r\n",
        "**Do all similar documents belong to the same class? Why or why not?**\r\n",
        "\r\n",
        "First of all, to remember, the first half of documents are from the negative reviews, and the other half from the positive reviews. With that said, all of the top 50 documents belongs to the same class because, normally positive and negative reviews have something in common (Like the words they use, for example, on negative reviews we can find the words \"sucks, worse, disappointed, etc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZDWeGs27MrB"
      },
      "source": [
        "##**EXERCISE 7**\r\n",
        "\r\n",
        "**7. (20 points)** Using Spacyâ€™s part of speech tagger, process all sentences (hint: donâ€™t forget to split the reviews) and count how many NOUN and VERB tags are found in all the movies review (TRAINING folder) separating them by label. In other words, how many NOUN and VERB tags are found in positive reviews, and how many NOUN and VERB tags are found in negative reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "00XzNrPe7tn4",
        "outputId": "df80b594-61be-4173-8a1f-cae8f6f36ce4"
      },
      "source": [
        "import spacy\r\n",
        "nlp = spacy.load(\"en_core_web_sm\")\r\n",
        "\r\n",
        "#Splitting the reviews\r\n",
        "#When we obtained the files from the zip file, the data variable was stored in the following order:\r\n",
        "#    *The first half are negative reviews\r\n",
        "#    *The last half are positive reviews\r\n",
        "total_elements = len(data)\r\n",
        "half = int(total_elements/2)\r\n",
        "negative_data = data[:half]\r\n",
        "positive_data = data[total_elements-half:]\r\n",
        "\r\n",
        "#Counting VERBS and NOUNS in the negative reviews\r\n",
        "print(\"Counting verbs and nouns in the negative reviews..\")\r\n",
        "total_verbs_negative_reviews = 0\r\n",
        "total_nouns_negative_reviews = 0\r\n",
        "total_negative_data = len(negative_data)\r\n",
        "last_percentage = 0\r\n",
        "\r\n",
        "nlp_negative_reviews = []\r\n",
        "for i in range(total_negative_data):\r\n",
        "    nlp_negative_reviews.append(nlp(negative_data[i]))\r\n",
        "    for token in nlp_negative_reviews[i]:\r\n",
        "        if token.pos_ == \"VERB\":\r\n",
        "            total_verbs_negative_reviews += 1\r\n",
        "        if token.pos_ == \"NOUN\":\r\n",
        "            total_nouns_negative_reviews += 1\r\n",
        "\r\n",
        "    percentage = (i/total_negative_data)*100\r\n",
        "\r\n",
        "    if percentage - last_percentage >= 5:\r\n",
        "        print(\"Progress: \"+str(round(percentage))+\"%\")\r\n",
        "        last_percentage = percentage\r\n",
        "\r\n",
        "\r\n",
        "#Counting VERBS and NOUNS in the positive reviews\r\n",
        "print(\"Counting verbs and nouns in the positive reviews..\")\r\n",
        "total_verbs_positive_reviews = 0\r\n",
        "total_nouns_positive_reviews = 0\r\n",
        "total_positive_data = len(positive_data)\r\n",
        "last_percentage = 0\r\n",
        "\r\n",
        "nlp_positive_reviews = []\r\n",
        "for i in range(total_positive_data):\r\n",
        "    nlp_positive_reviews.append(nlp(positive_data[i]))\r\n",
        "    for token in nlp_positive_reviews[i]:\r\n",
        "        if token.pos_ == \"VERB\":\r\n",
        "            total_verbs_positive_reviews += 1\r\n",
        "        if token.pos_ == \"NOUN\":\r\n",
        "            total_nouns_positive_reviews += 1\r\n",
        "    \r\n",
        "    percentage = (i/total_positive_data)*100\r\n",
        "\r\n",
        "    if percentage - last_percentage >= 5:\r\n",
        "        print(\"Progress: \"+str(round(percentage))+\"%\")\r\n",
        "        last_percentage = percentage\r\n",
        "\r\n",
        "print(\"Success!\")\r\n",
        "clear_output()\r\n",
        "\r\n",
        "#Passing the results to a pandas dataframe\r\n",
        "df_st = pd.DataFrame(columns=['Type of Reviews','Total of Verbs','Total of Nouns'])\r\n",
        "df_st.loc[len(df_st), :] = ['Negative',total_verbs_negative_reviews,total_nouns_negative_reviews]\r\n",
        "df_st.loc[len(df_st), :] = ['Positive',total_verbs_positive_reviews,total_nouns_positive_reviews]\r\n",
        "df_st"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type of Reviews</th>\n",
              "      <th>Total of Verbs</th>\n",
              "      <th>Total of Nouns</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Negative</td>\n",
              "      <td>355559</td>\n",
              "      <td>528475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Positive</td>\n",
              "      <td>342125</td>\n",
              "      <td>542978</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Type of Reviews Total of Verbs Total of Nouns\n",
              "0        Negative         355559         528475\n",
              "1        Positive         342125         542978"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc8Kguql7h7W"
      },
      "source": [
        "Answer the following questions: \r\n",
        "\r\n",
        "**When comparing both, do you see any differences?**\r\n",
        "\r\n",
        "There is relatively no abysmal difference between the positive and negative reviews, regarding the number of verbs and nouns in each one. We can see that the negative reviews have slightly more verbs, and on the other hand, the positive reviews have slighty more nouns.\r\n",
        "\r\n",
        "**Why do you think about the differences? Or lack of them.**\r\n",
        "\r\n",
        "The little difference between these results may be because, if we look at the size (in bytes) of both data sets, they are similar (around 37MB), so we can expect that there could be very little difference in the amount of tokens for each dataset (negative and positive). In other words, if any of the datasets (either positive or negative) are a lot larger, probably we would see a great difference comparing the results (number of nouns and verbs) to the other dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdccEDGtGhEG"
      },
      "source": [
        "##**EXERCISE 8**\r\n",
        "\r\n",
        "**8. (20 points)** Using the results from the PoS process in question 7, count how many different PUNCT tags are found and their respective counts from the full dataset provided (both negative and positives together). Using regex, write a set of regular expressions that generate the same counts from the dataset without using NLTK or Spacy, just regex.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY3iaZ9usp67"
      },
      "source": [
        "**Counting the PUNCT tags using the results from the previous question**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KOMfI6tGtMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e0f0f4d-c7d4-4183-ffb1-1433b6c23c43"
      },
      "source": [
        "nlp_reviews = nlp_negative_reviews + nlp_positive_reviews\r\n",
        "\r\n",
        "\r\n",
        "total_punct = 0\r\n",
        "print(\"Counting the punctuations in the reviews using last's exercise results..\")\r\n",
        "#Counting for PUNCT tags on each review (positive and negative)\r\n",
        "for i in range(len(nlp_reviews)):\r\n",
        "    for token in nlp_reviews[i]:\r\n",
        "        if token.pos_ == \"PUNCT\":\r\n",
        "            total_punct += 1\r\n",
        "\r\n",
        "print(\"Total PUNCT tags found in the full dataset (negative and positive reviews): \\033[1m\"+str(total_punct)+\"\\033[0m\")\r\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting the punctuations in the reviews using last's exercise results..\n",
            "Total PUNCT tags found in the full dataset (negative and positive reviews): \u001b[1m834967\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KvDr8eluHfS"
      },
      "source": [
        "**Counting the PUCT tags using regular expressions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU7hTGtMuMO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe52a63-acc0-4387-84c0-5d1f099a328b"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "print(\"Counting the punctuations in the reviews using regexp..\")\r\n",
        "total_punct = 0\r\n",
        "for i in range(len(data)):\r\n",
        "\r\n",
        "    #In theory, a sentence should begin with a capital letter, although, the reviews may contain some typos\r\n",
        "    #Basically, at least, the first letter of the sentence should be uppercase (At least one ocurrence '{1}')\r\n",
        "    #We are asumming the sentence with punctuation mark can end either with ?,! or .\r\n",
        "    #Also there could be colon, semicolon, quotation, single quotation, comma\r\n",
        "    #hyphen, dash, parenthesis, brackets, and braces inside a sentence\r\n",
        "    total_punct += len(re.findall('^[A-Z]{1}[^!?.]*[!?.]$', data[i]))\r\n",
        "    total_punct += len(re.findall(',', data[i]))\r\n",
        "    total_punct += len(re.findall(':', data[i]))\r\n",
        "    total_punct += len(re.findall(';', data[i]))\r\n",
        "    total_punct += len(re.findall('\"', data[i]))\r\n",
        "    total_punct += len(re.findall('\\'', data[i]))\r\n",
        "    total_punct += len(re.findall('/', data[i]))\r\n",
        "    total_punct += len(re.findall('{', data[i]))\r\n",
        "    total_punct += len(re.findall('}', data[i]))\r\n",
        "    total_punct += len(re.findall('\\[', data[i]))\r\n",
        "    total_punct += len(re.findall('\\]', data[i]))\r\n",
        "    total_punct += len(re.findall('\\(', data[i]))\r\n",
        "    total_punct += len(re.findall('\\)', data[i]))\r\n",
        "    total_punct += len(re.findall('-', data[i]))\r\n",
        "    total_punct += len(re.findall('â€”', data[i]))\r\n",
        "\r\n",
        "print(\"Total PUNCT tags matched using regex in the full dataset (negative and positive reviews): \\033[1m\"+str(total_punct)+\"\\033[0m\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting the punctuations in the reviews using regexp..\n",
            "Total PUNCT tags matched using regex in the full dataset (negative and positive reviews): \u001b[1m740746\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia_U4UQmGswN"
      },
      "source": [
        "**Can you get the same counts? If not, why do you think this is?**\r\n",
        "\r\n",
        "No, as we can see from the results in the last two questions, the previous exercise did a better job than the current exercise with regular expressions. The reason is because that there could be some inconsistencies in the full dataset.\r\n",
        "\r\n",
        "As we know, these reviews could be written by any kind of people, so, we can expect some gramatical errors, thus the results using regexp could be even worse than from the previous exercise.\r\n",
        "\r\n",
        "For example, let's say a sentence from a review starts with a lowercase letter (which is not suppose to be in that way), in this case, using the exercise from the previous question would be able to detect that tag correctly. However, the regexp won't be able to detect that match, because, according to the pattern, the sentence should start with an uppercase letter.\r\n",
        "\r\n",
        "Another example, let's say in a review, there's the following word \"apple's\". Using the previous exercise, the apostrophe would be tagged as PART. However, using the regex from the current exercise will be able to match that pattern as a punctuation tag (in which is not correct at all).\r\n",
        "\r\n",
        "Also another example, let's say in a review, have the following part in the text \"Mr. Doe\", using the previous exercise, the word \"Mr.\" will be tagged as PROPN. However, using the regex from the current exercise, it will be count as a punctuation tag (in which is not correct at all)."
      ]
    }
  ]
}